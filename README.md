# **Local PDF RAG Solution**

#### **Project Overview**
This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using **LangChain**, **ChromaDB**, and **Llama 3** via **Ollama**. It allows you to:
1. **Load and split a PDF** into text chunks.
2. **Store the chunks** in ChromaDB (a vector database) with embeddings generated by a Hugging Face model.
3. **Query ChromaDB** for relevant chunks based on a userâ€™s input.
4. **Generate responses** using Llama 3, a large language model, based on the retrieved chunks.

This solution is ideal for building **local, privacy-preserving question-answering systems** over PDF documents without relying on external APIs.

---

## **Key Features**
- **PDF Processing**:
  - Loads a PDF and splits it into smaller text chunks using `RecursiveCharacterTextSplitter`.
  - Extracts metadata such as **page numbers** and **document title** for better context.
- **Embedding and Storage**:
  - Generates embeddings for the text chunks using `HuggingFaceEmbeddings`.
  - Stores the embeddings and metadata in **ChromaDB** for efficient retrieval.
- **Querying and Response Generation**:
  - Retrieves relevant chunks from ChromaDB using **similarity search**.
  - Generates responses using **Llama 3** via **Ollama**.
- **Customizable**:
  - Adjustable **chunk size**, **chunk overlap**, and number of retrieved chunks (`k`).
  - Persistent storage of ChromaDB data for reuse across sessions.

---

## **How It Works**
1. **Embedding PDFs**:
   - The script loads a PDF, splits it into chunks, and stores the chunks in ChromaDB.
2. **Querying ChromaDB**:
   - The user inputs a query, and the script retrieves the most relevant chunks from ChromaDB.
3. **Generating Responses**:
   - The script uses **Llama 3** to generate a response based on the retrieved chunks.

---

## **Getting Started**

### **Prerequisites**
- Python 3.8+
- Ollama installed and running locally (for Llama 3).

### **Installation**
1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/local-pdf-rag.git
   cd local-pdf-rag
   ```
2. Install the dependencies:
   ```bash
   pip install langchain chromadb sentence-transformers ollama
   ```

### **Usage**
1. **Embed a PDF and Query**:
   ```bash
   python app.py --pdf_path example.pdf --persist_directory my_chroma_db
   ```
2. **Query an Existing ChromaDB**:
   ```bash
   python app.py --persist_directory my_chroma_db
   ```
3. **Interact with the Assistant**:
   - Enter your query at the prompt.
   - Type `exit` or `quit` to end the session.

---

## **Example**
```bash
You: What is the main topic of the document?
Assistant: The main topic of the document is...
```

---

## **Customization**
- **Chunk Size and Overlap**:
  - Adjust the `--chunk_size` and `--chunk_overlap` arguments to control how the PDF is split.
  ```bash
  python script.py --pdf_path example.pdf --chunk_size 500 --chunk_overlap 100
  ```
- **Number of Chunks**:
  - Modify the `k` parameter in `query_chromadb` to retrieve more or fewer chunks.
- **Persist Directory**:
  - Specify a custom directory for ChromaDB data using the `--persist_directory` argument.
  ```bash
  python script.py --pdf_path example.pdf --persist_directory custom_chroma_db
  ```

---

## **Dependencies**
- **[LangChain](https://www.langchain.com/)**: For document loading, text splitting, and vector storage.
- **[ChromaDB](https://www.chromadb.com/)**: For storing and retrieving embeddings.
- **[Hugging Face Embeddings](https://huggingface.co/)**: For generating embeddings.
- **[Ollama](https://ollama.ai/)**: For running Llama 3 locally.

---

## **License**
This project is open-source and available under the **MIT License**.

---

## **Contributing**
Contributions are welcome! Please follow these steps:
1. Fork the repository.
2. Create a new branch (`git checkout -b feature/YourFeature`).
3. Commit your changes (`git commit -m 'Add some feature'`).
4. Push to the branch (`git push origin feature/YourFeature`).
5. Open a pull request.

---

## **Acknowledgments**
- **[LangChain](https://www.langchain.com/)** for providing the framework for building RAG pipelines.
- **[ChromaDB](https://www.chromadb.com/)** for efficient vector storage and retrieval.
- **[Ollama](https://ollama.ai/)** for making Llama 3 accessible locally.

---

## **Support**
If you encounter any issues or have questions, please open an issue on the [GitHub repository](https://github.com/yourusername/local-pdf-rag/issues).

---

This README provides a **clear, structured, and engaging overview** of the project, its features, and how to use it. Let me know if youâ€™d like to add or modify anything! ðŸš€